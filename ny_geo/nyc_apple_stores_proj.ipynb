{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decc2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "598c75b4",
   "metadata": {},
   "source": [
    "# Analysis of Potential Store Locations\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook analyzes potential store locations based on data obtained from loopnet.com and the census website.\n",
    "\n",
    "### Goals\n",
    "- Determine the suitability of potential store locations.\n",
    "- Analyze the correlation between various factors and suitability.\n",
    "\n",
    "## Data Overview\n",
    "- **Data Sources:**\n",
    "  - *Addresses:* Open addresses for rent obtained from loopnet.com.\n",
    "  - *Existing Apple Stores Addresses:* obtained from apple.com.\n",
    "  - *Income:* Income data obtained from the census website.\n",
    "\n",
    "- **Initial Variables:**\n",
    "  - `Address`: Address of the potential location.\n",
    "  - `City`: City where the potential location is located.\n",
    "  - `ZIP`: ZIP code of the potential location.\n",
    "  - `Year Built`: Year the building was constructed.\n",
    "  - `SF`: Square footage of the potential location.\n",
    "  - `Price`: Rental price of the potential location.\n",
    "\n",
    "- **Derived Variables:**\n",
    "  - `potential_location`: Binary variable indicating suitability of a location.\n",
    "  - `nearest_distance`: Distance to the nearest existing store.\n",
    "  - `weighted_avg_income`: Weighted average income in the area.\n",
    "  - `yearly_price_per_SF`: Yearly price per square foot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba52427",
   "metadata": {},
   "source": [
    "\n",
    "## Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "6ce35e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from IPython.display import IFrame\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2dbc6",
   "metadata": {},
   "source": [
    "## Function to Read Text File Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "0c5854a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_file_content(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read().splitlines()\n",
    "    return content\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'nyc_rentals_ret.txt'  # loopnet addresses text file\n",
    "lines_text = (text_file_content(file_path))\n",
    "# Create a DataFrame\n",
    "labels = ['Address', 'City and ZIP', 'Year Built', 'SF', 'Price']\n",
    "\n",
    "def extract_info(text_lines):\n",
    "    var_list = []\n",
    "    for j in range(1,6):\n",
    "        var_list.append([text_lines[i] for i in range(j,len(text_lines), 6)]) \n",
    "    return var_list\n",
    "\n",
    "sublists_vals = (extract_info(lines_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21711ab",
   "metadata": {},
   "source": [
    "## Display Initial Dataframe and Variables Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "04a0f81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Address</th>\n",
       "      <th>City and ZIP</th>\n",
       "      <th>Year Built</th>\n",
       "      <th>SF</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2586 Linden Blvd</td>\n",
       "      <td>Brooklyn, NY, 11208</td>\n",
       "      <td>Built in 2015</td>\n",
       "      <td>8,500 SF Retail Space</td>\n",
       "      <td>$48.00 SF/YR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103 Macdougal St</td>\n",
       "      <td>New York, NY, 10012</td>\n",
       "      <td>Built in 1900</td>\n",
       "      <td>5,000 - 12,000 SF Retail Spaces</td>\n",
       "      <td>$120.00 SF/YR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>336 W 23rd St</td>\n",
       "      <td>New York, NY, 10011</td>\n",
       "      <td>Built in 1910</td>\n",
       "      <td>2,000 - 6,200 SF Retail Spaces</td>\n",
       "      <td>48.00 - $49.00 SF/YR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Wall Street</td>\n",
       "      <td>New York, NY, 10005</td>\n",
       "      <td>Built in 1904</td>\n",
       "      <td>5,000 - 12,500 SF Retail Spaces</td>\n",
       "      <td>84.00 - $88.00 SF/YR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31 W 21st St</td>\n",
       "      <td>New York, NY, 10010</td>\n",
       "      <td>Built in 1908</td>\n",
       "      <td>9,000 SF Retail Spaces</td>\n",
       "      <td>$115.00 SF/YR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>625-649 Eighth Ave</td>\n",
       "      <td>New York, NY, 10018</td>\n",
       "      <td>Built in 1950</td>\n",
       "      <td>196 - 29,905 SF Retail Spaces</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>4473 Amboy Rd</td>\n",
       "      <td>Staten Island, NY, 10312</td>\n",
       "      <td>Built in 2022</td>\n",
       "      <td>2,000 - 15,000 SF Retail Space</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>143 Fulton St</td>\n",
       "      <td>New York, NY, 10038</td>\n",
       "      <td>Built in 2018</td>\n",
       "      <td>570 - 8,444 SF Retail Spaces</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>768 5th Ave</td>\n",
       "      <td>New York, NY, 10019</td>\n",
       "      <td>Built in 1907</td>\n",
       "      <td>440 - 32,940 SF Retail Spaces</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>413 W 14th St</td>\n",
       "      <td>New York, NY, 10014</td>\n",
       "      <td>Built in 1929</td>\n",
       "      <td>1,519 - 23,215 SF Retail Spaces</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Address              City and ZIP     Year Built  \\\n",
       "0       2586 Linden Blvd       Brooklyn, NY, 11208  Built in 2015   \n",
       "1       103 Macdougal St       New York, NY, 10012  Built in 1900   \n",
       "2          336 W 23rd St       New York, NY, 10011  Built in 1910   \n",
       "3          1 Wall Street       New York, NY, 10005  Built in 1904   \n",
       "4           31 W 21st St       New York, NY, 10010  Built in 1908   \n",
       "..                   ...                       ...            ...   \n",
       "506  625-649 Eighth Ave        New York, NY, 10018  Built in 1950   \n",
       "507        4473 Amboy Rd  Staten Island, NY, 10312  Built in 2022   \n",
       "508        143 Fulton St       New York, NY, 10038  Built in 2018   \n",
       "509          768 5th Ave       New York, NY, 10019  Built in 1907   \n",
       "510        413 W 14th St       New York, NY, 10014  Built in 1929   \n",
       "\n",
       "                                  SF                 Price  \n",
       "0              8,500 SF Retail Space          $48.00 SF/YR  \n",
       "1    5,000 - 12,000 SF Retail Spaces         $120.00 SF/YR  \n",
       "2     2,000 - 6,200 SF Retail Spaces  48.00 - $49.00 SF/YR  \n",
       "3    5,000 - 12,500 SF Retail Spaces  84.00 - $88.00 SF/YR  \n",
       "4             9,000 SF Retail Spaces         $115.00 SF/YR  \n",
       "..                               ...                   ...  \n",
       "506    196 - 29,905 SF Retail Spaces                  none  \n",
       "507   2,000 - 15,000 SF Retail Space                  none  \n",
       "508     570 - 8,444 SF Retail Spaces                  none  \n",
       "509    440 - 32,940 SF Retail Spaces                  none  \n",
       "510  1,519 - 23,215 SF Retail Spaces                  none  \n",
       "\n",
       "[511 rows x 5 columns]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(sublists_vals, index=labels).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c6533",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6f2ab",
   "metadata": {},
   "source": [
    "The dataset consists of a DataFrame with 511 entries and 5 columns:\n",
    "\n",
    "- **Address**: Object type, 511 non-null values.\n",
    "- **City and ZIP**: Object type, 511 non-null values.\n",
    "- **Year Built**: Object type, 511 non-null values.\n",
    "- **SF**: Object type, 511 non-null values.\n",
    "- **Price**: Object type, 511 non-null values.\n",
    "\n",
    "It's important to note that these variables need to be converted to int or float for our analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2713cb",
   "metadata": {},
   "source": [
    "### Splitting 'City and ZIP' Column\n",
    "\n",
    "We split the 'City and ZIP' column into 'City' and 'ZIP' columns since we need the ZIP code for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36fad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['City', 'ZIP']] = df['City and ZIP'].str.split('NY, ', expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41238dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f98dbcd",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We performed several preprocessing steps on the dataset:\n",
    "\n",
    "1. Stripped whitespace from the 'Price' column.\n",
    "2. Replaced 'none' values in the 'Price' column with '- 0 SF/YR'.\n",
    "3. Extracted the yearly price per square foot ('yearly_price_per_SF') and square footage ('SF') from the 'Price' column.\n",
    "4. Extracted the year built ('year_built') from the 'Year Built' column.\n",
    "5. Removed extraneous information from the 'Address' column.\n",
    "\n",
    "Finally, we dropped the unnecessary columns 'Price' and 'Year Built'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price'] = df['Price'].str.strip()\n",
    "\n",
    "# Replace 'none' with '- 0 SF/YR'\n",
    "df['Price'] = df['Price'].replace('none', ' - 0 SF/YR')\n",
    "\n",
    "# Step 2 & 3: Extract substring between '-' or '$' and 'SF/YR' in the 'Price' column\n",
    "#df['yearly_price_per_SF'] = df['Price'].str.extract(r'(-|\\$)(.*?)(?= SF/YR)', expand=False)\n",
    "def get_substring(string, start_vals, end_vals):\n",
    "    max_indices_from_left = ([string.rfind(val) for val in start_vals])\n",
    "    max_index_from_left = max(max_indices_from_left) if max_indices_from_left else -1\n",
    "    truncate_left_side = string[max_index_from_left+len(string[max_index_from_left]):]\n",
    "    min_indices_from_right = [truncate_left_side.find(val) for val in end_vals]\n",
    "    min_index_from_right = min(min_indices_from_right) if min_indices_from_right else -1\n",
    "    truncate_right_side = truncate_left_side[:min_index_from_right]\n",
    "    return truncate_right_side #left_side #right_side[0] if right_side else left_side\n",
    "\n",
    "\n",
    "df['yearly_price_per_SF'] = df['Price'].apply(lambda x: get_substring(x, ['$', '-'], ['SF/YR']))\n",
    "df['SF'] = df['SF'].apply(lambda x: get_substring(x, ['-'], ['SF']))\n",
    "\n",
    "df['year_built'] = df['Year Built'].str.replace('Built in ', '')\n",
    "\n",
    "def remove_info(s):\n",
    "    return re.sub(r'^.*?-', '', s)\n",
    "\n",
    "\n",
    "df['Address'] = df['Address'].apply(remove_info)\n",
    "\n",
    "# Step 4: Drop unnecessary columns\n",
    "df.drop(columns=[ 'Price', 'Year Built'], inplace=True)\n",
    "\n",
    "df['full_address'] = df['Address'] + ' ' + df['City and ZIP']\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34063f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df['full_address'] = df['Address'] + ' ' + df['City and ZIP']\n",
    "df.head(20)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bfbf3",
   "metadata": {},
   "source": [
    "### Convert Text Coordinates to Tuples\n",
    "\n",
    "The function below takes a list of string coordinates and converts them into a list of tuple coordinates. It reads coordinates from a text file and parses them using the `ast.literal_eval` function. The resulting list contains tuples representing the coordinates.\n",
    "\n",
    "**Parameters:**\n",
    "- `string_coords_from_text` (list): A list of string coordinates.\n",
    "\n",
    "**Returns:**\n",
    "- `coords_from_text_tuples` (list): A list of tuple coordinates.\n",
    "\n",
    "Example usage:\n",
    "```python\n",
    "coords_from_text = text_file_content('geospatial_coords_nyc.txt')\n",
    "coord_tuples = text_to_coords(coords_from_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# Read coordinates from the text file\n",
    "coords_from_text = (text_file_content('geospatial_coords_nyc.txt'))\n",
    "\n",
    "# Convert string coordinates to tuples\n",
    "def text_to_coords(string_coords_from_text):\n",
    "    last_coord = [ast.literal_eval(string_coords_from_text[-1])]\n",
    "    coords_from_text_tuples = [ast.literal_eval(str_coord)[0]\n",
    "                               for str_coord in coords_from_text][:-1]+last_coord\n",
    "    return coords_from_text_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f417cdda",
   "metadata": {},
   "source": [
    "### Extracting Coordinates from Text and Adding to DataFrame \n",
    "\n",
    "The code snippet below extracts coordinates from a text file using the `text_to_coords` function and adds them to a DataFrame. The `text_to_coords` function converts string coordinates to tuple coordinates, which are then unpacked into separate lists for latitude and longitude. These lists are then added as new columns 'latitude' and 'longitude' to the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2403b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_from_text_tuples = text_to_coords(coords_from_text)\n",
    "\n",
    "coords_list_lat, coords_list_long = list(map(list, zip(*coords_from_text_tuples)))\n",
    "df['latitude'] = coords_list_lat\n",
    "df['longitude'] = coords_list_long\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbfa937",
   "metadata": {},
   "source": [
    "### Processing NYC Income Data \n",
    "\n",
    "Here below the NYC income data from an Excel file is extracted and converted into a pandas DataFrame. It then preprocesses the data by removing rows with missing values, creating a new column 'avg_income' with specified values based on income ranges, and converting the 'ZIP' column to string type while removing everything after '.'.\n",
    "\n",
    "Next, it filters the DataFrame to keep only the rows where the ZIP code is in the NYC list, as many ZIP codes are from the entire state of New York. It calculates the total income for each ZIP code by multiplying the average income by the number of earners and groups the data by ZIP code, summing the total income and total number of earners.\n",
    "\n",
    "Finally, it calculates the weighted average income for each ZIP code and resets the index to make 'zip_code' a column again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the nyc income Excel file into a pandas DataFrame\n",
    "df_income = pd.read_excel(r'nyc_income_by_zip.xlsx')\n",
    "# Remove rows with missing values (NaN)\n",
    "df_income.dropna(inplace=True)\n",
    "# Create a new column 'avg_income' with specified values (repeating pattern) based on the provided income range\n",
    "df_income['avg_income'] = [17, 37, 63, 87, 150, 250] * 1535\n",
    "# Convert 'zip_code' column to string type and remove everything after '.'\n",
    "df_income['ZIP'] = df_income['ZIP'].astype(str).apply(lambda x: x.split('.')[0])\n",
    "# Filter the DataFrame to keep rows where the zip_code is in the nyc list (plenty are from the whole state of NY)\n",
    "df_income = df_income[df_income['ZIP'].isin(list(df.ZIP))] \n",
    "# Calculate the total income for each zip code by multiplying the average income by the number of earners\n",
    "df_income['total_income'] = df_income['avg_income'] * df_income['nr_earners']\n",
    "# Group by 'zip_code' and sum the total income and total number of earners\n",
    "grouped_data = df_income.groupby('ZIP').agg({'total_income': 'sum', 'nr_earners': 'sum'})\n",
    "# Calculate the weighted average income for each zip code\n",
    "grouped_data['weighted_avg_income'] = grouped_data['total_income'] / grouped_data['nr_earners']\n",
    "# Reset index to make 'zip_code' a column again\n",
    "grouped_data = grouped_data.reset_index()\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49cb60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep rows where the zip_code is in the nyc list (plenty are from the whole state of NY)\n",
    "df = df[df['ZIP'].isin(list(grouped_data.ZIP))] \n",
    "# Merge the new DataFrame with the old DataFrame on the zip code column\n",
    "df = pd.merge(df, grouped_data[['ZIP', 'weighted_avg_income']], on='ZIP', how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373f750",
   "metadata": {},
   "source": [
    "### Calculating Nearest Distance to Apple Stores \n",
    "\n",
    "The code snippet below calculates the nearest distance from each coordinate in a DataFrame to a list of Apple store coordinates. It first reads the Apple store coordinates from a text file and converts them into a cleaned list of tuples.\n",
    "\n",
    "Then, it iterates over each coordinate in the DataFrame and calculates the distance to each Apple store coordinate using the geodesic distance method from the geopy library. The minimum distance is updated if necessary.\n",
    "\n",
    "Finally, it assigns the minimum distance to a new column named 'nearest_distance' in the DataFrame and displays the DataFrame with the nearest distances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5a1efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "# Read coordinates from the text file\n",
    "apple_stores_coords = (text_file_content('apple_store_locs_nyc.txt'))\n",
    "last_coord = [ast.literal_eval(apple_stores_coords[-1])]\n",
    "apple_stores_coords_from_text_tuples = [ast.literal_eval(str_coord)[0] for str_coord in apple_stores_coords][:-1]+last_coord\n",
    "\n",
    "\n",
    "# Assume your DataFrame containing coordinates is named df\n",
    "# Iterate over each coordinate in your DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    min_distance = float('inf')  # Initialize with infinity\n",
    "    # Iterate over each coordinate from the text file\n",
    "    for coord in apple_stores_coords_from_text_tuples:\n",
    "        # Calculate distance between coordinates\n",
    "        distance = geodesic((row['latitude'], row['longitude']), coord).meters\n",
    "        # Update minimum distance if necessary\n",
    "        min_distance = min(min_distance, distance)\n",
    "    # Assign minimum distance to a new column in your DataFrame\n",
    "    df.at[index, 'nearest_distance'] = min_distance\n",
    "\n",
    "# Display DataFrame with nearest distances\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cc262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric characters and convert to float\n",
    "df['SF'] = df['SF'].str.replace(',', '').str.strip().astype(float)\n",
    "\n",
    "# convert price to float\n",
    "df['yearly_price_per_SF'] = df['yearly_price_per_SF'].astype(float)\n",
    "\n",
    "# Convert 'year_built' column to integers\n",
    "df['year_built'] = df['year_built'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb465cd9",
   "metadata": {},
   "source": [
    "### Creating Binary Variable for Potential Location \n",
    "\n",
    "Here we create a binary variable named 'potential_location' in the DataFrame based on specified criteria. \n",
    "\n",
    "1. The distance to the nearest Apple store is compared to the 25th percentile distance among all locations.\n",
    "2. The weighted average income in the area is compared to the 25th percentile income among all locations.\n",
    "3. The square footage of the potential location is compared to the 25th percentile square footage among all locations.\n",
    "\n",
    "These criteria are combined using logical AND operations to determine potential locations. If a location satisfies all criteria, it is labeled as a potential location with a value of 1; otherwise, it is labeled as 0. The resulting binary variable is added to the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74518f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary variables based on criteria\n",
    "potential_location_distance = df['nearest_distance'] >= np.quantile(df.nearest_distance, .25)\n",
    "potential_location_income = df['weighted_avg_income'] >= np.quantile(df.weighted_avg_income, 0.25)\n",
    "potential_location_sf = df['SF'] >= np.quantile(df.SF, 0.25)\n",
    "#potential_location_price = df['yearly_price_per_SF'] <= np.quantile(df.yearly_price_per_SF, .95)\n",
    "#potential_location_year = df['year_built'] >= np.median(df.year_built)\n",
    "\n",
    "# Combine criteria\n",
    "potential_location_combined = (\n",
    "    potential_location_distance & \n",
    "    potential_location_income & \n",
    "    potential_location_sf \n",
    "    #potential_location_price \n",
    "   #& potential_location_year\n",
    ")\n",
    "\n",
    "# Label dataframe based on combined criteria\n",
    "df['potential_location'] = potential_location_combined.astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494f088",
   "metadata": {},
   "source": [
    "### Creating and Displaying Map of Potential Locations \n",
    "\n",
    "The map below visualizes the feasible and unfeasible locations for opening new Apple stores. \n",
    "\n",
    "1. The map is centered at the coordinates of New York City with a zoom level of 10.\n",
    "2. A MarkerCluster layer is added to group markers for potential locations.\n",
    "3. Markers are added to the map for each potential location in the DataFrame. If a location is labeled as a potential location (with a value of 1 in the 'potential_location' column), a green check icon is used; otherwise, a red cross icon is used.\n",
    "4. The map is saved as an HTML file named 'potential_locations_map.html'.\n",
    "5. Finally, the map is displayed directly in the Jupyter Notebook using an IFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5683a999",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_center = [40.7128, -74.0060]  # New York City coordinates\n",
    "map_zoom = 10  # Zoom level\n",
    "m = folium.Map(location=map_center, zoom_start=map_zoom)\n",
    "\n",
    "# MarkerCluster layer\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# Add markers for potential locations\n",
    "for index, row in df.iterrows():\n",
    "    if row['potential_location'] == 1:\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            popup=f\"Location: {index}\",\n",
    "            icon=folium.Icon(color='green', icon='check')\n",
    "        ).add_to(marker_cluster)\n",
    "    else:\n",
    "        folium.Marker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            popup=f\"Location: {index}\",\n",
    "            icon=folium.Icon(color='red', icon='times')\n",
    "        ).add_to(marker_cluster)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "m.save('potential_locations_map.html')\n",
    "\n",
    "# Display the map directly in the notebook\n",
    "IFrame(src='potential_locations_map.html', width=700, height=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9881d",
   "metadata": {},
   "source": [
    "\n",
    "### distribution of predictors across the target variable via histograms.\n",
    "\n",
    "1. The number of rows and columns for subplots is determined based on the number of predictors. Two additional rows are added to account for the legend and title.\n",
    "2. Subplots are created using the `plt.subplots` function with the specified number of rows and columns.\n",
    "3. The axes are flattened for easy iteration.\n",
    "4. Each predictor is plotted individually on a subplot using Seaborn's `histplot` function.\n",
    "5. To balance the classes of the target variable, the dataframe is randomly sampled. Half of the samples with the target variable equal to 0 are selected, and all samples with the target variable equal to 1 are included.\n",
    "6. Titles, x-labels, y-labels, and legends are set for each subplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e2cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine the number of rows and columns for subplots\n",
    "num_rows = (len(predictors) + 2) // 3  # Add 2 to account for the legend and title\n",
    "num_cols = min(len(predictors), 3)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plotting each predictor individually\n",
    "for i, predictor in enumerate(predictors):\n",
    "    ax = axes[i]\n",
    "    # Randomly sample the dataframe to balance the target variable classes\n",
    "    df_sampled = pd.concat([\n",
    "        df[df[target] == 0].sample(frac=0.5),\n",
    "        df[df[target] == 1].sample(frac=1)\n",
    "    ])\n",
    "    sns.histplot(data=df_sampled, x=predictor, hue=target, multiple='stack', bins=25, ax=ax)\n",
    "    ax.set_title(f'Distribution of {target} Across {predictor}', fontsize=6)  # Adjust the title fontsize\n",
    "    ax.set_xlabel(predictor)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(title=target)\n",
    "\n",
    "# Remove empty subplots\n",
    "for i in range(len(predictors), num_rows * num_cols):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364802a",
   "metadata": {},
   "source": [
    "The histogram analysis reveals an interesting trend in the distribution of categories across different frequencies. Specifically, as the frequency of occurrences increases, the distribution tends to favor category 0 slightly more. Conversely, when the frequency decreases, category 1 becomes more prevalent in the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbdf7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = predictors  # Replace with the column names you want to select\n",
    "filtered_df = df[selected_columns]\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e96bbb",
   "metadata": {},
   "source": [
    "###  Machine Learning Algorithms Selection and Motivation:\n",
    "\n",
    "1. **Logistic Regression**:\n",
    "   - **Motivation**: Logistic Regression is a simple and interpretable algorithm that is well-suited for binary classification tasks.\n",
    "   - **Strengths**: Easy to implement, computationally efficient, provides probabilities for predictions, interpretable coefficients.\n",
    "   - **Use Cases**: Often used as a baseline model for binary classification tasks, especially when interpretability is important.\n",
    "\n",
    "2. **Random Forest**:\n",
    "   - **Motivation**: Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to improve accuracy and reduce overfitting.\n",
    "   - **Strengths**: High accuracy, robust to overfitting, handles both numerical and categorical data, provides feature importances.\n",
    "   - **Use Cases**: Commonly used for classification and regression tasks where high accuracy is desired, and interpretability is not a primary concern.\n",
    "\n",
    "3. **Support Vector Machine (SVM)**:\n",
    "   - **Motivation**: SVM is a powerful algorithm for classification tasks, particularly when dealing with complex, nonlinear decision boundaries.\n",
    "   - **Strengths**: Effective in high-dimensional spaces, versatile (can use different kernel functions), robust to overfitting in high-dimensional spaces.\n",
    "   - **Use Cases**: Used for both classification and regression tasks, especially when the data is not linearly separable or when dealing with small to medium-sized datasets.\n",
    "\n",
    "4. **Gradient Boosting**:\n",
    "   - **Motivation**: Gradient Boosting is an ensemble learning technique that builds a strong predictive model by combining multiple weak models (typically decision trees) sequentially.\n",
    "   - **Strengths**: High predictive accuracy, handles complex relationships between features, robust to outliers, automatically handles missing data.\n",
    "   - **Use Cases**: Widely used in both regression and classification tasks, particularly when dealing with structured/tabular data and when model interpretability is not a primary concern.\n",
    "\n",
    "5. **Multilayer Perceptron (MLP)**:\n",
    "   - **Motivation**: MLP is a type of artificial neural network composed of multiple layers of nodes (neurons) that can learn complex patterns in data.\n",
    "   - **Strengths**: Can capture complex relationships in data, suitable for nonlinear problems, performs well with large datasets, can learn feature hierarchies automatically.\n",
    "   - **Use Cases**: Used for a wide range of tasks, including classification, regression, and pattern recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = filtered_df\n",
    "y = df['potential_location']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "logreg = LogisticRegression()\n",
    "rf = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "gbm = GradientBoostingClassifier()\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Train models\n",
    "logreg.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "gbm.fit(X_train, y_train)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "logreg_preds = logreg.predict(X_test)\n",
    "rf_preds = rf.predict(X_test)\n",
    "svm_preds = svm.predict(X_test)\n",
    "gbm_preds = gbm.predict(X_test)\n",
    "mlp_preds = mlp.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "models = [logreg, rf, svm, gbm, mlp]\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'SVM', 'Gradient Boosting', 'MLP']\n",
    "\n",
    "\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    preds_train, preds_test = model.predict(X_train), model.predict(X_test) \n",
    "    accuracy_train, accuracy_test = accuracy_score(y_train, preds_train), accuracy_score(y_test, preds_test)\n",
    "    precision_train, precision_test = precision_score(y_train, preds_train), precision_score(y_test, preds_test)\n",
    "    recall_train, recall_test = recall_score(y_train, preds_train), recall_score(y_test, preds_test)\n",
    "    f1_train, f1_test = f1_score(y_train, preds_train), f1_score(y_test, preds_test)\n",
    "    roc_auc_train, roc_auc_test = roc_auc_score(y_train, preds_train), roc_auc_score(y_test, preds_test) \n",
    "    \n",
    "    print(f\"Performance metrics for {name}:\")\n",
    "    print(\"accuracy train {tr}, accuracy test {test}\".format(tr= accuracy_train, test = accuracy_test))\n",
    "    print(\"precision train {tr}, precision test {test}\".format(tr= precision_train, test = accuracy_test))\n",
    "    print(\"recall train {tr}, recall test {test}\".format(tr= recall_train, test = recall_test))\n",
    "    print(\"f1 train {tr}, f1 test {test}\".format(tr= f1_train, test = recall_test))\n",
    "    print(\"roc auc train {tr}, roc auc test {test}\".format(tr= roc_auc_train, test = recall_test))\n",
    "    print()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c3391",
   "metadata": {},
   "source": [
    "\n",
    "### Analysis of Model Performance Metrics\n",
    "\n",
    "#### Logistic Regression:\n",
    "- **Accuracy:** \n",
    "  - Train: 0.6972\n",
    "  - Test: 0.6566\n",
    "- **Precision:** \n",
    "  - Train: 0.6712\n",
    "  - Test: 0.6566\n",
    "- **Recall:** \n",
    "  - Train: 0.3403\n",
    "  - Test: 0.2059\n",
    "- **F1 Score:** \n",
    "  - Train: 0.4516\n",
    "  - Test: 0.2059\n",
    "- **ROC AUC:** \n",
    "  - Train: 0.6220\n",
    "  - Test: 0.2059\n",
    "\n",
    "Logistic Regression achieves moderate accuracy but has relatively low precision and recall, indicating that it may not effectively identify potential locations. The F1 score suggests a balance between precision and recall, but the ROC AUC score indicates only slightly better-than-random performance in distinguishing between potential and non-potential locations.\n",
    "\n",
    "#### Random Forest:\n",
    "- **Accuracy:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **Precision:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **Recall:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **F1 Score:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **ROC AUC:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "\n",
    "Random Forest achieves perfect scores across all metrics, indicating flawless performance\n",
    "\n",
    "#### Support Vector Machine (SVM):\n",
    "- **Accuracy:** \n",
    "  - Train: 0.7048\n",
    "  - Test: 0.6768\n",
    "- **Precision:** \n",
    "  - Train: 0.7917\n",
    "  - Test: 0.6768\n",
    "- **Recall:** \n",
    "  - Train: 0.2639\n",
    "  - Test: 0.1176\n",
    "- **F1 Score:** \n",
    "  - Train: 0.3958\n",
    "  - Test: 0.1176\n",
    "- **ROC AUC:** \n",
    "  - Train: 0.6119\n",
    "  - Test: 0.1176\n",
    "\n",
    "SVM achieves moderate accuracy and precision, but its recall is quite low, indicating that it fails to identify many potential locations. The F1 score is also relatively low, suggesting a trade-off between precision and recall. The ROC AUC score indicates slightly better-than-random performance.\n",
    "\n",
    "#### Gradient Boosting:\n",
    "- **Accuracy:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **Precision:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **Recall:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **F1 Score:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "- **ROC AUC:** \n",
    "  - Train: 1.0000\n",
    "  - Test: 1.0000\n",
    "\n",
    "Gradient Boosting achieves perfect scores across all metrics, together with Random Forest it is so far the best model of the tried ones \n",
    "\n",
    "#### Multilayer Perceptron (MLP):\n",
    "- **Accuracy:** \n",
    "  - Train: 0.7099\n",
    "  - Test: 0.6869\n",
    "- **Precision:** \n",
    "  - Train: 0.8750\n",
    "  - Test: 0.6869\n",
    "- **Recall:** \n",
    "  - Train: 0.2431\n",
    "  - Test: 0.1471\n",
    "- **F1 Score:** \n",
    "  - Train: 0.3804\n",
    "  - Test: 0.1471\n",
    "- **ROC AUC:** \n",
    "  - Train: 0.6115\n",
    "  - Test: 0.1471\n",
    "\n",
    "MLP achieves good accuracy and precision, with moderate recall. The F1 score indicates a balance between precision and recall, and the ROC AUC score suggests reasonable discrimination ability. However, the model's performance on recall and F1 score is lower compared to the gradient boosting especially when considering the F1 score since the accuracy is not totally reliable given the unbalanced distribution of the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae5cb27",
   "metadata": {},
   "source": [
    "Here is a visual representation of some of the accuracy metrics for each ML model. It's easy to detect that both Random Forest and Gradient Boosting performed impeccably given the absence of false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea036d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train each model and make predictions\n",
    "model_names = ['Logistic Regression', 'Random Forest', 'SVM', 'Gradient Boosting', 'MLP']\n",
    "\n",
    "# Create subplots with 3 rows and 2 columns\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (model, name) in enumerate(zip(models, model_names)):\n",
    "    preds_test = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, preds_test)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', ax=axes[i])\n",
    "    axes[i].set_title(f'Confusion Matrix - {name}')\n",
    "    axes[i].set_xlabel('Predicted')\n",
    "    axes[i].set_ylabel('True')\n",
    "\n",
    "# Remove any extra empty subplots\n",
    "for j in range(len(model_names), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222fb24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
